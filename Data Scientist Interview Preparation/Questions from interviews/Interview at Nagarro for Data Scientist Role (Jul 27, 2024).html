<head>
    <script crossorigin="anonymous" integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4="
        src="https://code.jquery.com/jquery-3.2.1.min.js"></script>
 
    <!-- Google AdSense Using Machine Learning Code -->
    <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    <script>
        (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-3071098372371409",
            enable_page_level_ads: true
        });
    </script>

    <script>
        $(document).ready(function () {
            $.ajax({
                url: "https://raw.githubusercontent.com/ashishjain1547/pubLessonsInTechnology/main/links_to_tech_clubs.json",
                success: function (result) {
                    let grouplink = JSON.parse(result)['Beta Tech Club'];
                    $("#customWhatsAppGroupLinkWrapper").html(
                        `
                        <h2 class="custom_link_h2"><a href="${grouplink}" target="_blank"> 
                            <span>Join us on:</span>
                            <span class="customLink"><i class="fa fa-whatsapp"></i> Whatsapp </span>
                            </a>
                        </h2>
                        `
                    );
                }
            });
        });
    </script>

    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">

    <style>
        pre {
            white-space: pre-wrap;
            white-space: -moz-pre-wrap;
            white-space: -pre-wrap;
            white-space: -o-pre-wrap;
            word-wrap: break-word;
        }

        .customLink {
            background-color: #4CAF50;
            border: none;
            color: white !important;
            padding: 8px 13px;
            text-align: center;
            text-decoration: none;
            display: inline-block;
            font-size: 14px;
            margin: 4px 2px;
            cursor: pointer;
        }

        .customLink:hover {
            text-decoration: none;
        }

        div.code-block-decoration.footer {
            display: none;
        }

        button.export-sheets-button-wrapper {
            display: none;
        }
    </style>

    <style>
        .custom_link_h2 a {
            color: black;
            text-decoration: none;
            text-align: center;
        }

        .custom_link_h2 a:hover {
            color: black;
        }

        .custom_link_h2 a:active {
            color: black;
        }

        .custom_link_h2 span {
            translate: 0px -5px;
            display: inline-block;
        }

        .custom_link_h2 img {
            width: 100px;
            padding: 0px;
            border: none;
            box-shadow: none;
        }
    </style>
    <style>
        .customul {
            list-style: none;
        }

        [aria-hidden='true'] {
            display: none;
        }
    </style>

    <style>
        i.ir {
            color: red;
        }

        p.pb {
            color: blue;
        }
    </style>

</head>

<div id="customWhatsAppGroupLinkWrapper"></div>

Find out more: <a class="customLink"
    href="https://survival8.blogspot.com/2024/04/index-for-job-interviews-preparation.html"
    target="_blank">Index For Interviews Preparation For Data Scientist Role
</a>
<br>

<div class="separator" style="clear: both;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjJfmLMq8wiZEmI0r4-GFMZJ37ZVJP3v_irbYl8BJ2mgYsMMExDnmFH6j_OQVfoKzmqhIHemvQQWjoq8zbR9_hpuu4EO6bCj-GASzg2-ptKENmt-T3BAKeg9t923MU8QCvzgbtjcpPhyphenhyphenCLt4GhkA0N6vY174Wynav5eQgCGEaSXIN0fVAE0FhXbkUjRYQWJ/s224/nagarro.png" style="display: block; padding: 1em 0; text-align: center; "><img alt="" border="0" width="600" data-original-height="224" data-original-width="224" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjJfmLMq8wiZEmI0r4-GFMZJ37ZVJP3v_irbYl8BJ2mgYsMMExDnmFH6j_OQVfoKzmqhIHemvQQWjoq8zbR9_hpuu4EO6bCj-GASzg2-ptKENmt-T3BAKeg9t923MU8QCvzgbtjcpPhyphenhyphenCLt4GhkA0N6vY174Wynav5eQgCGEaSXIN0fVAE0FhXbkUjRYQWJ/s600/nagarro.png"/></a></div>

<pre>Q1: Tell about yourself.

Q2: What all Python packages you have worked on?

Q3: What are embeddings?

<span>Embeddings are a type of representation for text, images, or other data types that map high-dimensional inputs into lower-dimensional vector spaces. In the context of natural language processing (NLP) and machine learning, embeddings are particularly useful because they enable the representation of words, phrases, or even entire documents as dense vectors of real numbers. These vectors capture semantic meaning in such a way that similar concepts are located near each other in the embedding space.

<h3>Key Concepts of Embeddings</h3>

Dimensionality Reduction:

Embeddings reduce the dimensionality of data while preserving meaningful relationships. For example, words in a high-dimensional vocabulary are mapped to lower-dimensional vectors.

Semantic Representation:

Embeddings capture semantic similarities. Words with similar meanings are represented by vectors that are close to each other in the embedding space. For instance, the words "king" and "queen" might be close in the vector space.

Contextual Information:

Advanced embeddings like those produced by models such as BERT or GPT capture contextual information, meaning the representation of a word can change based on the surrounding words.

<h3>Types of Embeddings</h3>

<b>Word Embeddings:</b>

Word2Vec: <b>Trains shallow neural networks to predict the context given the word (Skip-gram) or predict a word given the context (CBOW - Continuous Bag of Words).
<i>Trick to remember: C for 'given the Context' and C for CBOW.</i></b>

GloVe (Global Vectors for Word Representation): Combines the benefits of local context window methods and global matrix factorization methods.
FastText: An extension of Word2Vec that considers subword information, making it robust to out-of-vocabulary words.

<b>Contextual Embeddings:</b>

BERT (Bidirectional Encoder Representations from Transformers): Generates embeddings that consider the context of a word within a sentence. The same word can have different embeddings based on its usage.

GPT (Generative Pre-trained Transformer): Produces embeddings that are used for generating text based on a given context.

<b>Sentence and Document Embeddings:</b>

Doc2Vec: Extends Word2Vec to create vectors for entire documents.

Universal Sentence Encoder: Produces embeddings for sentences and paragraphs, designed to capture the meaning of longer text segments.

<b>Image Embeddings:</b>

Used in computer vision to represent images as vectors. These embeddings are typically generated using convolutional neural networks (CNNs).

<h3>Applications of Embeddings</h3>

Text Similarity and Search:

Embeddings are used to find similar documents or sentences by comparing their vector representations. For example, in search engines, embeddings can improve the relevance of search results.

Machine Translation:

Embeddings facilitate the translation of text from one language to another by capturing the semantic meaning of words and phrases.

Recommendation Systems:

User and item embeddings can be used to recommend products, movies, or music based on similarities in the embedding space.

Sentiment Analysis:

Embeddings can help in understanding the sentiment of a piece of text by capturing the nuanced meaning of words and phrases.</span>

~~~

Q4: What is the difference between one-hot encoding and embeddings?

<span>One-hot encoding and embeddings are both techniques used to represent categorical data, particularly in natural language processing (NLP) and machine learning. However, they differ significantly in their methodology, storage efficiency, and ability to capture semantic meaning.

<h3>One-Hot Encoding</h3>

Definition:
One-hot encoding is a representation of categorical variables as binary vectors. Each category or word is represented by a vector where only one element is "hot" (i.e., set to 1) and all other elements are "cold" (i.e., set to 0).

Characteristics:

Dimensionality:

The length of each one-hot vector equals the number of unique categories in the dataset. For a vocabulary of size V, each one-hot vector is of length V.
Sparsity:

One-hot vectors are sparse, containing mostly zeros except for a single 1.
No Semantic Information:

One-hot encoding does not capture any semantic relationships between categories. Each category is equidistant from every other category.
Memory Inefficiency:

For large vocabularies, one-hot encoding can be very memory-intensive due to the high dimensionality and sparsity.
Example:
For a vocabulary of {apple, banana, orange}:

apple: [1, 0, 0]
banana: [0, 1, 0]
orange: [0, 0, 1]

Definition:
Embeddings are dense vector representations of categorical data. Each category or word is mapped to a vector of fixed size, typically much smaller than the size of the vocabulary. Embeddings are learned from data and can capture semantic relationships.

<h3>Characteristics</h3>

Dimensionality:

The length of each embedding vector is much smaller than the number of unique categories, typically ranging from 50 to 300 dimensions.

Density:

Embedding vectors are dense, with most elements being non-zero.

Semantic Information:

Embeddings capture semantic meanings and relationships. Words with similar meanings have similar vectors (i.e., are close in the embedding space).

Memory Efficiency:

Embeddings are more memory-efficient compared to one-hot encoding due to their lower dimensionality.
Example:
For a vocabulary of {apple, banana, orange}, with embeddings of size 3:

apple: [0.1, 0.3, 0.5]
banana: [0.2, 0.4, 0.6]
orange: [0.0, 0.2, 0.4]


<table><thead><tr><th>Aspect</th><th>One-Hot Encoding</th><th>Embeddings</th></tr></thead><tbody><tr><td><strong>Dimensionality</strong></td><td>Size of vocabulary (V)</td><td>Fixed size (e.g., 50-300)</td></tr><tr><td><strong>Vector Type</strong></td><td>Sparse</td><td>Dense</td></tr><tr><td><strong>Semantic Information</strong></td><td>No</td><td>Yes</td></tr><tr><td><strong>Memory Efficiency</strong></td><td>Low (due to high dimensionality)</td><td>High (due to lower dimensionality)</td></tr><tr><td><strong>Training Requirement</strong></td><td>None (deterministic)</td><td>Requires training (or pre-trained)</td></tr><tr><td><strong>Relationship Capture</strong></td><td>No relationships between categories</td><td>Captures relationships between words</td></tr></tbody></table>

<h3>Use Cases</h3>

One-Hot Encoding:

Suitable for categorical variables with a small number of unique categories.
Often used in traditional machine learning algorithms that cannot directly handle categorical variables (e.g., decision trees, linear models).

Embeddings:

Suitable for large vocabularies, especially in NLP tasks.
Used in deep learning models such as neural networks for tasks like text classification, machine translation, and sentiment analysis.
Commonly used pre-trained embeddings include Word2Vec, GloVe, and embeddings from transformer models like BERT and GPT. </span>

~~~

Q5: What is Count Vectorizer?

<span>The Count Vectorizer is a technique used to convert a collection of text documents into a matrix of token counts. It is a simple and commonly used method for feature extraction in natural language processing (NLP) and text analysis. The idea is to represent each document as a vector of word counts, which can then be used as input for various machine learning algorithms.

<h3>How Count Vectorizer Works</h3>

Tokenization:

The text documents are tokenized, meaning they are split into individual words or tokens.

Building the Vocabulary:

A vocabulary (or dictionary) is built from the unique tokens across the entire corpus of documents.

Counting Tokens:

For each document, the number of occurrences of each token in the vocabulary is counted.

Vector Representation:

Each document is represented as a vector of token counts, where each element of the vector corresponds to the count of a specific token from the vocabulary.

<b>Example</b>

Suppose we have the following documents:

"I love programming"
"Programming is fun"
"I love machine learning"
The Count Vectorizer would perform the following steps:

Tokenization:

Document 1: ["I", "love", "programming"]
Document 2: ["Programming", "is", "fun"]
Document 3: ["I", "love", "machine", "learning"]
Building the Vocabulary:

Vocabulary: {"I", "love", "programming", "is", "fun", "machine", "learning"}
Counting Tokens:

Document 1: [1, 1, 1, 0, 0, 0, 0]
Document 2: [0, 0, 1, 1, 1, 0, 0]
Document 3: [1, 1, 0, 0, 0, 1, 1]

<h3>Advantages of Count Vectorizer</h3>

Simplicity:

Easy to understand and implement.
Efficiency:

Suitable for many text classification tasks.

Foundation for Other Techniques:

Forms the basis for more advanced techniques like TF-IDF (Term Frequency-Inverse Document Frequency).

<h3>Limitations of Count Vectorizer</h3>

Sparsity:

The resulting vectors can be very sparse, especially for large vocabularies.
Ignoring Context:

It does not consider the context or order of words, only their frequency.
High Dimensionality:

The size of the vectors increases with the size of the vocabulary, which can lead to high-dimensional data that is computationally expensive to process.
No Semantic Understanding:

Similar words or synonyms are treated as entirely separate features.
Despite these limitations, Count Vectorizer is a valuable tool for initial text processing and feature extraction in NLP pipelines.</span>

~~~

Q6: Brief about Word2Vec?

<span>Word2Vec is a popular technique for natural language processing (NLP) that involves training a shallow neural network model to learn dense vector representations of words. These vector representations, known as word embeddings, capture the semantic relationships between words in a continuous vector space. Developed by a team of researchers led by Tomas Mikolov at Google in 2013, Word2Vec has been influential in advancing the field of NLP.

<h3>Key Concepts of Word2Vec</h3>

Distributed Representations:

Unlike traditional one-hot encoding, which represents words as sparse vectors, Word2Vec represents words as dense vectors of real numbers. This allows the model to capture more information about the relationships between words.
Semantic Similarity:

Words with similar meanings are located close to each other in the vector space. For example, the words "king" and "queen" might have vectors that are close together, reflecting their semantic similarity.
Contextual Information:

Word2Vec captures the context of words in a corpus, which helps in understanding their meanings based on surrounding words.

<h3>Training Word2Vec</h3>

Word2Vec can be trained using two main approaches:

Continuous Bag of Words (CBOW):

In the CBOW model, the objective is to predict a target word given its context (the surrounding words). This model averages the context word vectors to predict the target word.

Skip-gram:

In the Skip-gram model, the objective is to predict the context words given a target word. This model is particularly effective for smaller datasets and captures more fine-grained information about word relationships.

<h3>Applications of Word2Vec</h3>

Similarity and Analogy Tasks:

Word2Vec can be used to find similar words and perform analogy tasks. For example, finding the word that is to "queen" as "king" is to "man".
Text Classification:

Embeddings from Word2Vec can be used as features in text classification tasks, such as sentiment analysis or spam detection.

<h3>Advantages of Word2Vec</h3>

Semantic Understanding:

Captures semantic relationships between words, making it useful for various NLP tasks.
Efficient Training:

Word2Vec is computationally efficient and can be trained on large datasets relatively quickly.
Flexibility:

Can be fine-tuned for specific tasks and integrated into larger machine learning pipelines.

<h3>Limitations of Word2Vec</h3>

Context Independence:

Word2Vec generates a single vector representation for each word, regardless of its context in a sentence. This can be a limitation for words with multiple meanings (polysemy).
Static Embeddings:

The embeddings are static once trained and do not change with new data unless retrained.
Requires Large Datasets:

To capture meaningful relationships, Word2Vec requires a large corpus for training.</span>

~~~

Q7: Brief about the Architecture of Word2Vec.

<span><b>Skip-Gram model:</b>

<div class="separator" style="clear: both;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjYTmdvNVAz6zBe_m1odhXfoN8ozufTCzSyYkt9F-CHhf42IRtecVNF9Kzyv5nBtQHQ7jGZn8anpJWCJ1YXqXzHEUyfFi8nROl3nyqj6cHmw682hwq59Efu_idQ9Pv4zBVQfMJ1jbQFU_zqw_M2PqIa_6A7LfGHCdRKMvgkTMicXp_1H86wzb1gLYYjHA/s783/skip-gram.png" style="display: block; padding: 1em 0; text-align: center; "><img alt="" border="0" data-original-height="336" data-original-width="783" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjYTmdvNVAz6zBe_m1odhXfoN8ozufTCzSyYkt9F-CHhf42IRtecVNF9Kzyv5nBtQHQ7jGZn8anpJWCJ1YXqXzHEUyfFi8nROl3nyqj6cHmw682hwq59Efu_idQ9Pv4zBVQfMJ1jbQFU_zqw_M2PqIa_6A7LfGHCdRKMvgkTMicXp_1H86wzb1gLYYjHA/s600/skip-gram.png" width="600"></a></div>

<div class="separator" style="clear: both;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgUzWo7icnOi_0INN80XQMcFlIq6W4z8HdJh1ULbcFJgWsXgP6kxjADEuKFoCqyqVDg45_9qRJuOsCoYq8wOG0YC5RW2fZGo9dRTpQ-CjgkXHw8mChAs1A2FWEoYL5bUTdkvFeJdlKeA-RMdEoGWWmJ-ngc3zak0UbkehWBK4kZrN4_1WCtRf4ZS-8eQg/s960/skip-gram%20training.png" style="display: block; padding: 1em 0; text-align: center; "><img alt="" border="0" data-original-height="461" data-original-width="960" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgUzWo7icnOi_0INN80XQMcFlIq6W4z8HdJh1ULbcFJgWsXgP6kxjADEuKFoCqyqVDg45_9qRJuOsCoYq8wOG0YC5RW2fZGo9dRTpQ-CjgkXHw8mChAs1A2FWEoYL5bUTdkvFeJdlKeA-RMdEoGWWmJ-ngc3zak0UbkehWBK4kZrN4_1WCtRf4ZS-8eQg/s600/skip-gram%20training.png" width="600"></a></div>

<div class="separator" style="clear: both;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi6ntDKTFXBj7cB2VVnjvPOHU5yktoc-Dbcomi4hKFWunZnMBPMyyUPhQ52ID2UV1kk0hyrQVpSj5EdyFNlEPMS8kW3nCVHrrT3aqJ3_1LaLfbMhWHht3y1IvROWZ_35RuNUovjBERlwAY8dq6dFE3kp-qDeL2ZkikYDu40t5DDO_yEkxtBOKWUjIC66A/s940/skip-gram%20training%202.png" style="display: block; padding: 1em 0; text-align: center; "><img alt="" border="0" data-original-height="511" data-original-width="940" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi6ntDKTFXBj7cB2VVnjvPOHU5yktoc-Dbcomi4hKFWunZnMBPMyyUPhQ52ID2UV1kk0hyrQVpSj5EdyFNlEPMS8kW3nCVHrrT3aqJ3_1LaLfbMhWHht3y1IvROWZ_35RuNUovjBERlwAY8dq6dFE3kp-qDeL2ZkikYDu40t5DDO_yEkxtBOKWUjIC66A/s600/skip-gram%20training%202.png" width="600"></a></div>

<b>Continuous Bag of Words:</b>

<div class="separator" style="clear: both;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiNSVnr0hoexSp1pbKzRNiKHQ2EiSRMa9S-keRJhVd3YSTl589Z94Mq31zvU-Ka0-NIeDy1TvTXlnQEKhCw1Ca_Bd9y_4zit5kXMP4WMAn4JU3R5-dXSfoUpVhhWbAskhJAFOi4pBZOL7Yp8M7pDvzarHzJmhTvAtK2dCQW8oYS-MEmD1iof_s2h-JPbA/s748/CBOW%205%20Grams.png" style="display: block; padding: 1em 0; text-align: center; "><img alt="" border="0" data-original-height="330" data-original-width="748" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiNSVnr0hoexSp1pbKzRNiKHQ2EiSRMa9S-keRJhVd3YSTl589Z94Mq31zvU-Ka0-NIeDy1TvTXlnQEKhCw1Ca_Bd9y_4zit5kXMP4WMAn4JU3R5-dXSfoUpVhhWbAskhJAFOi4pBZOL7Yp8M7pDvzarHzJmhTvAtK2dCQW8oYS-MEmD1iof_s2h-JPbA/s600/CBOW%205%20Grams.png" width="600"></a></div>

<div class="separator" style="clear: both;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj0u_Nza77_dbrvX1u0woXMVXxhXAT7nG14yymNwAPzyooGXJ3cYBEeCXKCELzJWBttQJfGmidsdVJMVg_zAxaRs2XIXinBep9-FfRNqkaZQz1YFia5U3_dIh9C1LHXCqAQVGmQmgcFeAPi-ppXtGquXoY-iOqaNRk7LcgBlDbgI6up8C86wAAH4-CU-w/s944/CBOW%20word2vec%20network.png" style="display: block; padding: 1em 0; text-align: center; "><img alt="" border="0" data-original-height="557" data-original-width="944" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj0u_Nza77_dbrvX1u0woXMVXxhXAT7nG14yymNwAPzyooGXJ3cYBEeCXKCELzJWBttQJfGmidsdVJMVg_zAxaRs2XIXinBep9-FfRNqkaZQz1YFia5U3_dIh9C1LHXCqAQVGmQmgcFeAPi-ppXtGquXoY-iOqaNRk7LcgBlDbgI6up8C86wAAH4-CU-w/s600/CBOW%20word2vec%20network.png" width="600"></a></div> </span>

~~~

Q8: Brief about Anomaly detection and following three types of models that you tried:
- Auto encoder
- K-medians 
- Isolation Forest

~~~

Q9: Provide the End-to-end project description for the Anomaly Detection project.

~~~

Q10: Is it correct to say if maximise negative of log-likelihood of an observation to detect anomalies?

<span>Maximizing the negative log-likelihood of an observation is not typically used for anomaly detection. Instead, minimizing the negative log-likelihood is more common in training machine learning models. To detect anomalies, one would often look for observations that have low likelihoods (or high negative log-likelihoods) under a given model, rather than maximizing the negative log-likelihood.

Understanding Negative Log-Likelihood (NLL)
Log-Likelihood:

In a probabilistic model, the likelihood of an observation is the probability of the observation given the parameters of the model.
The log-likelihood is simply the natural logarithm of the likelihood. Taking the logarithm helps in dealing with very small probability values and can simplify the differentiation process during optimization.
Negative Log-Likelihood:

The negative log-likelihood (NLL) is the negative of the log-likelihood. Minimizing the NLL is equivalent to maximizing the likelihood.
In many machine learning models, such as neural networks, we minimize the NLL during training to find the best model parameters.
Anomaly Detection
In anomaly detection, the goal is to identify observations that deviate significantly from the normal behavior captured by the model. Here’s how the concept of log-likelihood can be applied:

Train a Probabilistic Model:

Train a model on normal (non-anomalous) data to capture the distribution of normal behavior.
Compute Log-Likelihoods:

For each new observation, compute the log-likelihood under the trained model.
Identify Anomalies:

Observations with low likelihoods (equivalently, high negative log-likelihoods) are considered anomalies. These are points that the model considers unlikely under the learned distribution of normal data.
Example
Consider a Gaussian Mixture Model (GMM) for anomaly detection:

Train GMM on Normal Data:

python
Copy code
from sklearn.mixture import GaussianMixture

# Assume X_train contains normal data
gmm = GaussianMixture(n_components=2).fit(X_train)
Compute Log-Likelihoods:

python
Copy code
# Assume X_test contains new observations
log_likelihoods = gmm.score_samples(X_test)
Detect Anomalies:

python
Copy code
# Set a threshold for anomaly detection
threshold = -10
anomalies = X_test[log_likelihoods &lt; threshold]
In this example, gmm.score_samples(X_test) returns the log-likelihood of each observation in X_test. Observations with log-likelihoods below the threshold are considered anomalies.

<h3>Summary</h3>

<b>Training Phase: Minimize negative log-likelihood to learn model parameters.

Detection Phase: Identify anomalies by looking for observations with low likelihoods (high negative log-likelihoods).</b>

Therefore, it is not correct to say that maximizing the negative log-likelihood of an observation is used to detect anomalies. Instead, we use the concept of likelihood (or log-likelihood) to find observations that are unlikely under the model and thus potential anomalies.</span>

~~~

Q11: What is ginie index?
<span>
<div class="markdown prose w-full break-words dark:prose-invert light"><p>The Gini index, also known as the Gini impurity, is a metric used to evaluate the quality of a split in decision tree algorithms, such as CART (Classification and Regression Trees). It measures the degree of impurity or diversity in a dataset. The lower the Gini index, the less impure the node is, meaning that it is more homogenous.</p><h3>Definition</h3><p>The Gini index for a node <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi></mrow><annotation encoding="application/x-tex">t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.6151em;"></span><span class="mord mathnormal">t</span></span></span></span> is defined as:</p><p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>G</mi><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo><mo>=</mo><mn>1</mn><mo>−</mo><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>C</mi></msubsup><msubsup><mi>p</mi><mi>i</mi><mn>2</mn></msubsup></mrow><annotation encoding="application/x-tex">G(t) = 1 - \sum_{i=1}^{C} p_i^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathnormal">G</span><span class="mopen">(</span><span class="mord mathnormal">t</span><span class="mclose">)</span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 0.7278em; vertical-align: -0.0833em;"></span><span class="mord">1</span><span class="mspace" style="margin-right: 0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right: 0.2222em;"></span></span><span class="base"><span class="strut" style="height: 1.2809em; vertical-align: -0.2997em;"></span><span class="mop"><span class="mop op-symbol small-op" style="position: relative; top: 0em;">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.9812em;"><span style="top: -2.4003em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top: -3.2029em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right: 0.07153em;">C</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.2997em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord"><span class="mord mathnormal">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.8141em;"><span style="top: -2.4413em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span><span style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.2587em;"><span></span></span></span></span></span></span></span></span></span></p><p>where:</p><ul><li><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>C</mi></mrow><annotation encoding="application/x-tex">C</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.6833em;"></span><span class="mord mathnormal" style="margin-right: 0.07153em;">C</span></span></span></span> is the number of classes.</li><li><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>p</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">p_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.625em; vertical-align: -0.1944em;"></span><span class="mord"><span class="mord mathnormal">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3117em;"><span style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span></span></span></span></span></span></span></span></span></span> is the proportion of instances of class <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi></mrow><annotation encoding="application/x-tex">i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.6595em;"></span><span class="mord mathnormal">i</span></span></span></span> in the node.</li></ul><h3>Interpretation</h3><ul><li><strong>Gini Index of 0:</strong><ul><li>Indicates a pure node, where all instances belong to a single class.</li></ul></li><li><strong>Gini Index of 0.5:</strong><ul><li>Indicates a completely impure node, where the instances are uniformly distributed across all classes (in the case of two classes).</li></ul></li></ul><h3>Example Calculation</h3><p>Suppose we have a node with the following class distribution:</p><ul><li>Class A: 3 instances</li><li>Class B: 2 instances</li></ul><p>The total number of instances <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex">N</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.6833em;"></span><span class="mord mathnormal" style="margin-right: 0.10903em;">N</span></span></span></span> is 5. The proportions are:</p><ul><li><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>p</mi><mi>A</mi></msub><mo>=</mo><mfrac><mn>3</mn><mn>5</mn></mfrac><mo>=</mo><mn>0.6</mn></mrow><annotation encoding="application/x-tex">p_A = \frac{3}{5} = 0.6</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.625em; vertical-align: -0.1944em;"></span><span class="mord"><span class="mord mathnormal">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3283em;"><span style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">A</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 1.1901em; vertical-align: -0.345em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.8451em;"><span style="top: -2.655em;"><span class="pstrut" style="height: 3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">5</span></span></span></span><span style="top: -3.23em;"><span class="pstrut" style="height: 3em;"></span><span class="frac-line" style="border-bottom-width: 0.04em;"></span></span><span style="top: -3.394em;"><span class="pstrut" style="height: 3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">3</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 0.6444em;"></span><span class="mord">0.6</span></span></span></span></li><li><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>p</mi><mi>B</mi></msub><mo>=</mo><mfrac><mn>2</mn><mn>5</mn></mfrac><mo>=</mo><mn>0.4</mn></mrow><annotation encoding="application/x-tex">p_B = \frac{2}{5} = 0.4</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.625em; vertical-align: -0.1944em;"></span><span class="mord"><span class="mord mathnormal">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3283em;"><span style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right: 0.05017em;">B</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 1.1901em; vertical-align: -0.345em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.8451em;"><span style="top: -2.655em;"><span class="pstrut" style="height: 3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">5</span></span></span></span><span style="top: -3.23em;"><span class="pstrut" style="height: 3em;"></span><span class="frac-line" style="border-bottom-width: 0.04em;"></span></span><span style="top: -3.394em;"><span class="pstrut" style="height: 3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 0.6444em;"></span><span class="mord">0.4</span></span></span></span></li></ul><p>The Gini index for this node is:</p><p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>G</mi><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo><mo>=</mo><mn>1</mn><mo>−</mo><mo stretchy="false">(</mo><mn>0.</mn><msup><mn>6</mn><mn>2</mn></msup><mo>+</mo><mn>0.</mn><msup><mn>4</mn><mn>2</mn></msup><mo stretchy="false">)</mo><mo>=</mo><mn>1</mn><mo>−</mo><mo stretchy="false">(</mo><mn>0.36</mn><mo>+</mo><mn>0.16</mn><mo stretchy="false">)</mo><mo>=</mo><mn>1</mn><mo>−</mo><mn>0.52</mn><mo>=</mo><mn>0.48</mn></mrow><annotation encoding="application/x-tex">G(t) = 1 - (0.6^2 + 0.4^2) = 1 - (0.36 + 0.16) = 1 - 0.52 = 0.48</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathnormal">G</span><span class="mopen">(</span><span class="mord mathnormal">t</span><span class="mclose">)</span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 0.7278em; vertical-align: -0.0833em;"></span><span class="mord">1</span><span class="mspace" style="margin-right: 0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right: 0.2222em;"></span></span><span class="base"><span class="strut" style="height: 1.0641em; vertical-align: -0.25em;"></span><span class="mopen">(</span><span class="mord">0.</span><span class="mord"><span class="mord">6</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.8141em;"><span style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right: 0.2222em;"></span></span><span class="base"><span class="strut" style="height: 1.0641em; vertical-align: -0.25em;"></span><span class="mord">0.</span><span class="mord"><span class="mord">4</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.8141em;"><span style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 0.7278em; vertical-align: -0.0833em;"></span><span class="mord">1</span><span class="mspace" style="margin-right: 0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right: 0.2222em;"></span></span><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mopen">(</span><span class="mord">0.36</span><span class="mspace" style="margin-right: 0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right: 0.2222em;"></span></span><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord">0.16</span><span class="mclose">)</span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 0.7278em; vertical-align: -0.0833em;"></span><span class="mord">1</span><span class="mspace" style="margin-right: 0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right: 0.2222em;"></span></span><span class="base"><span class="strut" style="height: 0.6444em;"></span><span class="mord">0.52</span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 0.6444em;"></span><span class="mord">0.48</span></span></span></span></p><h3>Usage in Decision Trees</h3><p>In decision tree algorithms, the Gini index is used to select the best feature to split the data at each node. The algorithm chooses the feature that minimizes the weighted sum of the Gini indices of the child nodes, effectively selecting the split that produces the most homogenous child nodes.</p><h3>Pseudocode for Gini Index in Decision Tree Split</h3><pre><div class="dark bg-gray-950 rounded-md border-[0.5px] border-token-border-medium"><div class="flex items-center relative text-token-text-secondary bg-token-main-surface-secondary px-4 py-2 text-xs font-sans justify-between rounded-t-md"><span>python</span><div class="flex items-center"><span class="" data-state="closed"><button class="flex gap-1 items-center"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewBox="0 0 24 24" class="icon-sm"><path fill="currentColor" fill-rule="evenodd" d="M7 5a3 3 0 0 1 3-3h9a3 3 0 0 1 3 3v9a3 3 0 0 1-3 3h-2v2a3 3 0 0 1-3 3H5a3 3 0 0 1-3-3v-9a3 3 0 0 1 3-3h2zm2 2h5a3 3 0 0 1 3 3v5h2a1 1 0 0 0 1-1V5a1 1 0 0 0-1-1h-9a1 1 0 0 0-1 1zM5 9a1 1 0 0 0-1 1v9a1 1 0 0 0 1 1h9a1 1 0 0 0 1-1v-9a1 1 0 0 0-1-1z" clip-rule="evenodd"></path></svg>Copy code</button></span></div></div><div class="overflow-y-auto p-4" dir="ltr"><code class="!whitespace-pre hljs language-python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">gini_index</span>(<span class="hljs-params">groups, classes</span>):
    <span class="hljs-comment"># Count all samples at split point</span>
    n_instances = <span class="hljs-built_in">float</span>(<span class="hljs-built_in">sum</span>([<span class="hljs-built_in">len</span>(group) <span class="hljs-keyword">for</span> group <span class="hljs-keyword">in</span> groups]))
    
    <span class="hljs-comment"># Sum the Gini index for each group</span>
    gini = <span class="hljs-number">0.0</span>
    <span class="hljs-keyword">for</span> group <span class="hljs-keyword">in</span> groups:
        size = <span class="hljs-built_in">float</span>(<span class="hljs-built_in">len</span>(group))
        <span class="hljs-comment"># Avoid division by zero</span>
        <span class="hljs-keyword">if</span> size == <span class="hljs-number">0</span>:
            <span class="hljs-keyword">continue</span>
        
        score = <span class="hljs-number">0.0</span>
        <span class="hljs-comment"># Score the group based on the score for each class</span>
        <span class="hljs-keyword">for</span> class_val <span class="hljs-keyword">in</span> classes:
            p = [row[-<span class="hljs-number">1</span>] <span class="hljs-keyword">for</span> row <span class="hljs-keyword">in</span> group].count(class_val) / size
            score += p * p
        
        <span class="hljs-comment"># Weight the group score by its relative size</span>
        gini += (<span class="hljs-number">1.0</span> - score) * (size / n_instances)
    
    <span class="hljs-keyword">return</span> gini
</code></div></div></pre><p>In this pseudocode:</p><ul><li><code>groups</code> is a list of groups formed by splitting the dataset on a particular feature.</li><li><code>classes</code> is a list of unique class values in the dataset.</li><li>The function calculates the Gini index for each group and returns the weighted sum of the Gini indices.</li></ul><h3>Conclusion</h3><p>The Gini index is a fundamental concept in decision tree learning, used to evaluate the quality of splits and guide the tree-building process. By selecting splits that minimize the Gini index, decision trees aim to create nodes that are as pure as possible, leading to more accurate and interpretable models.</p></div>
</span>

~~~

Q12: How do you do attribute selection for building a decision tree?

<span>
Attribute selection for building a decision tree involves choosing the most informative features to split the data at each node in the tree. The goal is to select attributes that maximize the purity of the child nodes, thus improving the decision tree's ability to correctly classify the data. The most commonly used criteria for attribute selection in decision trees are Gini Index, Information Gain, and Gain Ratio.

<h3>Steps in Attribute Selection</h3>

Calculate the Splitting Criteria for Each Attribute:

Gini Index: Measures the impurity of a dataset. Lower values indicate purer nodes.
Information Gain: Measures the reduction in entropy (disorder) after a dataset is split on an attribute.
Gain Ratio: Adjusts the information gain by taking into account the intrinsic information of a split, addressing the bias towards attributes with many values.
Choose the Attribute with the Best Split:

Select the attribute that either minimizes the Gini Index or maximizes the Information Gain or Gain Ratio.
Repeat the Process for Child Nodes:

Recursively apply the attribute selection process to the child nodes until stopping criteria are met (e.g., maximum tree depth, minimum number of samples per leaf, or no further information gain).
</span>

~~~

Q13: Which ones are faster Boosting algorithms or Bagging Algorithms?

<span>In general, Bagging algorithms tend to be faster than Boosting algorithms. Here’s a detailed comparison of both techniques in terms of speed and computational complexity:

<h3>Bagging Algorithms</h3>

Characteristics:
Parallel Training: Bagging (Bootstrap Aggregating) algorithms train multiple base learners (e.g., decision trees) independently in parallel. Each learner is trained on a different bootstrap sample of the training data.
Less Sequential Dependency: Since each base learner is trained independently, the overall training process can be parallelized, making it faster and more efficient.

Examples: Random Forest is a common example of a bagging algorithm.

<b>Speed:</b>

Training Speed: Bagging is generally faster because each model can be trained independently and in parallel.
Prediction Speed: Bagging can also be faster during prediction because the models are independent, and predictions from each model can be averaged in parallel.

<h3>Boosting Algorithms</h3>

<b>Characteristics:</b>

Sequential Training: Boosting algorithms train base learners sequentially. Each new learner is trained to correct the errors made by the previous learners.

High Sequential Dependency: The training of each base learner depends on the performance of the previous learners, leading to a more sequential and thus slower process.
Examples: AdaBoost, Gradient Boosting Machines (GBM), XGBoost, and LightGBM are common examples of boosting algorithms.

Speed:

Training Speed: Boosting is generally slower due to the sequential nature of training. Each new model needs to be trained based on the performance of the previous model, which prevents parallel training.
Prediction Speed: Boosting can also be slower during prediction because the predictions are made sequentially by each model in the boosting chain.

<h3>Detailed Comparison</h3>

1. Training Speed
Bagging: Faster training due to parallelism. All base models are trained independently, and the process can take advantage of multiple CPU cores or distributed computing.
Boosting: Slower training because models are built sequentially. Each model corrects the errors of the previous ones, preventing parallel training.

2. Prediction Speed
Bagging: Predictions can be made relatively quickly because each model's prediction can be computed in parallel and then averaged or voted upon.
Boosting: Predictions can be slower because they involve sequentially aggregating the results of all models in the boosting chain.

3. Scalability
Bagging: More scalable due to parallelizable nature, especially beneficial for large datasets and distributed computing environments.
Boosting: Less scalable due to the sequential dependency of the models.

<h3>Summary</h3>

Bagging algorithms (e.g., Random Forest) are generally faster in both training and prediction phases due to their parallelizable nature.
Boosting algorithms (e.g., XGBoost, LightGBM) are typically slower due to the sequential training process and the dependency of each model on the performance of the previous ones.

However, it’s important to note that boosting algorithms often achieve higher accuracy and better performance on certain tasks due to their sequential learning process and the focus on correcting errors from previous models. The choice between bagging and boosting should depend on the specific requirements of your problem, including the trade-offs between speed and accuracy.</span>

~~~

Q14: What is confusion matrix?

<span>A confusion matrix is a performance measurement tool for machine learning classification problems. It is a table that is used to describe the performance of a classification model (or "classifier") on a set of test data for which the true values are known. The confusion matrix itself is relatively simple to understand, but the terminology can be a bit confusing.

Components of a Confusion Matrix
A confusion matrix for a binary classifier is a 2x2 matrix with the following structure:

Predicted Positive	Predicted Negative
Actual Positive	True Positive (TP)	False Negative (FN)
Actual Negative	False Positive (FP)	True Negative (TN)
For a multi-class classification problem, the matrix would be larger, with dimensions corresponding to the number of classes.

Definitions
True Positive (TP): The number of correct positive predictions.
True Negative (TN): The number of correct negative predictions.
False Positive (FP): The number of incorrect positive predictions (also known as Type I error).
False Negative (FN): The number of incorrect negative predictions (also known as Type II error).</span>

~~~

Q15: When do you use Precision and when do you use Recall? Provide an example.

<span>Answer:

<div class="markdown prose w-full break-words dark:prose-invert light"><p>Precision and recall are two important metrics used to evaluate the performance of classification models, especially in contexts where class distribution is imbalanced or where the costs of false positives and false negatives differ significantly.</p><h3>Precision</h3><p><strong>Definition</strong>: Precision measures the proportion of true positive predictions out of all positive predictions made by the model.</p><p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>Precision</mtext><mo>=</mo><mfrac><mtext>True&nbsp;Positives</mtext><mrow><mtext>True&nbsp;Positives</mtext><mo>+</mo><mtext>False&nbsp;Positives</mtext></mrow></mfrac></mrow><annotation encoding="application/x-tex">\text{Precision} = \frac{\text{True Positives}}{\text{True Positives} + \text{False Positives}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.6833em;"></span><span class="mord text"><span class="mord">Precision</span></span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 1.2757em; vertical-align: -0.4033em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.8723em;"><span style="top: -2.655em;"><span class="pstrut" style="height: 3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">True&nbsp;Positives</span></span><span class="mbin mtight">+</span><span class="mord text mtight"><span class="mord mtight">False&nbsp;Positives</span></span></span></span></span><span style="top: -3.23em;"><span class="pstrut" style="height: 3em;"></span><span class="frac-line" style="border-bottom-width: 0.04em;"></span></span><span style="top: -3.394em;"><span class="pstrut" style="height: 3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">True&nbsp;Positives</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.4033em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></p><p><strong>When to Use</strong>:</p><ul><li><strong>High Cost of False Positives</strong>: When the cost of incorrectly labeling a negative instance as positive is high, precision is crucial. For example, in medical diagnostics, false positives (e.g., diagnosing a healthy person with a disease) can lead to unnecessary stress and additional testing.</li><li><strong>Precision-Oriented Tasks</strong>: When you need to ensure that the positive predictions you make are highly reliable.</li></ul><p><strong>Example</strong>: Suppose you are developing a spam email filter. Precision is important if you want to avoid marking legitimate emails as spam. A high precision means that when your model classifies an email as spam, it is likely to be spam.</p><h3>Recall</h3><p><strong>Definition</strong>: Recall measures the proportion of true positive predictions out of all actual positives in the dataset.</p><p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>Recall</mtext><mo>=</mo><mfrac><mtext>True&nbsp;Positives</mtext><mrow><mtext>True&nbsp;Positives</mtext><mo>+</mo><mtext>False&nbsp;Negatives</mtext></mrow></mfrac></mrow><annotation encoding="application/x-tex">\text{Recall} = \frac{\text{True Positives}}{\text{True Positives} + \text{False Negatives}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.6944em;"></span><span class="mord text"><span class="mord">Recall</span></span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 1.3534em; vertical-align: -0.4811em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.8723em;"><span style="top: -2.655em;"><span class="pstrut" style="height: 3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">True&nbsp;Positives</span></span><span class="mbin mtight">+</span><span class="mord text mtight"><span class="mord mtight">False&nbsp;Negatives</span></span></span></span></span><span style="top: -3.23em;"><span class="pstrut" style="height: 3em;"></span><span class="frac-line" style="border-bottom-width: 0.04em;"></span></span><span style="top: -3.394em;"><span class="pstrut" style="height: 3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">True&nbsp;Positives</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.4811em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></p><p><strong>When to Use</strong>:</p><ul><li><strong>High Cost of False Negatives</strong>: When missing positive instances is costly or dangerous, recall is crucial. For example, in cancer screening, a false negative (failing to detect a disease when it is present) can have severe consequences.</li><li><strong>Recall-Oriented Tasks</strong>: When you want to capture as many positive instances as possible, even if it means including some false positives.</li></ul><p><strong>Example</strong>: In a disease outbreak detection system, recall is crucial to ensure that as many actual cases as possible are detected, even if it means some healthy individuals might be incorrectly flagged as potentially infected.</p><h3>Example Scenario</h3><p><strong>Scenario</strong>: Consider a model designed to identify fraudulent transactions in a banking system.</p><ul><li><p><strong>Precision</strong>: If the model has high precision, it means that when it flags a transaction as fraudulent, it is likely to be fraudulent. This is important to reduce the number of legitimate transactions mistakenly flagged as fraud, which can cause inconvenience for customers.</p></li><li><p><strong>Recall</strong>: If the model has high recall, it means it detects a high proportion of all fraudulent transactions. This is important to ensure that as many fraudulent activities as possible are caught, even if it means flagging some legitimate transactions as fraudulent.</p></li></ul><h3>Trade-Off Between Precision and Recall</h3><p>In many cases, there is a trade-off between precision and recall. Improving one often reduces the other. For example, setting a higher threshold for classifying a transaction as fraudulent will increase precision (fewer legitimate transactions are incorrectly classified as fraud) but may decrease recall (some fraudulent transactions might be missed).</p><p><strong>Balancing Act</strong>:</p><ul><li><strong>F1 Score</strong>: To balance precision and recall, you can use the F1 score, which is the harmonic mean of precision and recall. It provides a single metric that considers both false positives and false negatives.</li></ul><p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>F1&nbsp;Score</mtext><mo>=</mo><mn>2</mn><mo>×</mo><mfrac><mrow><mtext>Precision</mtext><mo>×</mo><mtext>Recall</mtext></mrow><mrow><mtext>Precision</mtext><mo>+</mo><mtext>Recall</mtext></mrow></mfrac></mrow><annotation encoding="application/x-tex">\text{F1 Score} = 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.6833em;"></span><span class="mord text"><span class="mord">F1&nbsp;Score</span></span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 0.7278em; vertical-align: -0.0833em;"></span><span class="mord">2</span><span class="mspace" style="margin-right: 0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right: 0.2222em;"></span></span><span class="base"><span class="strut" style="height: 1.2834em; vertical-align: -0.4033em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.8801em;"><span style="top: -2.655em;"><span class="pstrut" style="height: 3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">Precision</span></span><span class="mbin mtight">+</span><span class="mord text mtight"><span class="mord mtight">Recall</span></span></span></span></span><span style="top: -3.23em;"><span class="pstrut" style="height: 3em;"></span><span class="frac-line" style="border-bottom-width: 0.04em;"></span></span><span style="top: -3.394em;"><span class="pstrut" style="height: 3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">Precision</span></span><span class="mbin mtight">×</span><span class="mord text mtight"><span class="mord mtight">Recall</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.4033em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></p><h3>Summary</h3><ul><li><strong>Use Precision</strong>: When false positives are costly and you want to ensure that positive predictions are reliable.</li><li><strong>Use Recall</strong>: When false negatives are costly and you want to capture as many positive instances as possible.</li></ul><p>Choosing the appropriate metric depends on the specific context and the costs associated with false positives and false negatives in your application.</p></div>
</span>

~~~

Q16: Calculate precision and recall for this given confusion matrix.

~~~

Q17: How many jumps it would take to climb a 100 storey building if you double your strength with every jump? And you start with 1 storey a jump.

<span>
<div class="markdown prose w-full break-words dark:prose-invert light"><p>To determine how many jumps it would take to climb a 100-storey building if you double your strength with each jump, starting with the ability to jump 1 storey on the first jump, we can use the concept of geometric progression.</p><h3>Understanding the Problem</h3><ol><li><strong>Initial Jump Strength</strong>: 1 storey (on the first jump)</li><li><strong>Doubling Factor</strong>: After each jump, the jumping strength doubles.</li></ol><h3>Calculation</h3><ul><li><strong>Jump 1</strong>: You jump 1 storey.</li><li><strong>Jump 2</strong>: You jump <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn><mo>×</mo><mn>2</mn><mo>=</mo><mn>2</mn></mrow><annotation encoding="application/x-tex">1 \times 2 = 2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.7278em; vertical-align: -0.0833em;"></span><span class="mord">1</span><span class="mspace" style="margin-right: 0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right: 0.2222em;"></span></span><span class="base"><span class="strut" style="height: 0.6444em;"></span><span class="mord">2</span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 0.6444em;"></span><span class="mord">2</span></span></span></span> storeys.</li><li><strong>Jump 3</strong>: You jump <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn><mo>×</mo><mn>2</mn><mo>=</mo><mn>4</mn></mrow><annotation encoding="application/x-tex">2 \times 2 = 4</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.7278em; vertical-align: -0.0833em;"></span><span class="mord">2</span><span class="mspace" style="margin-right: 0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right: 0.2222em;"></span></span><span class="base"><span class="strut" style="height: 0.6444em;"></span><span class="mord">2</span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 0.6444em;"></span><span class="mord">4</span></span></span></span> storeys.</li><li><strong>Jump 4</strong>: You jump <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>4</mn><mo>×</mo><mn>2</mn><mo>=</mo><mn>8</mn></mrow><annotation encoding="application/x-tex">4 \times 2 = 8</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.7278em; vertical-align: -0.0833em;"></span><span class="mord">4</span><span class="mspace" style="margin-right: 0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right: 0.2222em;"></span></span><span class="base"><span class="strut" style="height: 0.6444em;"></span><span class="mord">2</span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 0.6444em;"></span><span class="mord">8</span></span></span></span> storeys.</li></ul><p>And so on. The number of storeys you can jump after <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi></mrow><annotation encoding="application/x-tex">n</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.4306em;"></span><span class="mord mathnormal">n</span></span></span></span> jumps is given by <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mn>2</mn><mrow><mo stretchy="false">(</mo><mi>n</mi><mo>−</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msup></mrow><annotation encoding="application/x-tex">2^{(n-1)}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.888em;"></span><span class="mord"><span class="mord">2</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.888em;"><span style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight">n</span><span class="mbin mtight">−</span><span class="mord mtight">1</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span></span></span></span>.</p><p>To find the number of jumps required to reach or exceed 100 storeys, we need to find the smallest <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi></mrow><annotation encoding="application/x-tex">n</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.4306em;"></span><span class="mord mathnormal">n</span></span></span></span> such that:</p><p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>0</mn></mrow><mrow><mi>n</mi><mo>−</mo><mn>1</mn></mrow></msubsup><msup><mn>2</mn><mi>i</mi></msup><mo>≥</mo><mn>100</mn></mrow><annotation encoding="application/x-tex">\sum_{i=0}^{n-1} 2^i \geq 100</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 1.2537em; vertical-align: -0.2997em;"></span><span class="mop"><span class="mop op-symbol small-op" style="position: relative; top: 0em;">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.954em;"><span style="top: -2.4003em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">0</span></span></span></span><span style="top: -3.2029em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">n</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.2997em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord"><span class="mord">2</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.8247em;"><span style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">≥</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 0.6444em;"></span><span class="mord">100</span></span></span></span></p><p>The sum of the first <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi></mrow><annotation encoding="application/x-tex">n</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.4306em;"></span><span class="mord mathnormal">n</span></span></span></span> terms of a geometric series is:</p><p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>Sum</mtext><mo>=</mo><msup><mn>2</mn><mn>0</mn></msup><mo>+</mo><msup><mn>2</mn><mn>1</mn></msup><mo>+</mo><msup><mn>2</mn><mn>2</mn></msup><mo>+</mo><mo>…</mo><mo>+</mo><msup><mn>2</mn><mrow><mo stretchy="false">(</mo><mi>n</mi><mo>−</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msup><mo>=</mo><msup><mn>2</mn><mi>n</mi></msup><mo>−</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">\text{Sum} = 2^0 + 2^1 + 2^2 + \ldots + 2^{(n-1)} = 2^n - 1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.6833em;"></span><span class="mord text"><span class="mord">Sum</span></span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 0.8974em; vertical-align: -0.0833em;"></span><span class="mord"><span class="mord">2</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.8141em;"><span style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right: 0.2222em;"></span></span><span class="base"><span class="strut" style="height: 0.8974em; vertical-align: -0.0833em;"></span><span class="mord"><span class="mord">2</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.8141em;"><span style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right: 0.2222em;"></span></span><span class="base"><span class="strut" style="height: 0.8974em; vertical-align: -0.0833em;"></span><span class="mord"><span class="mord">2</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.8141em;"><span style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right: 0.2222em;"></span></span><span class="base"><span class="strut" style="height: 0.6667em; vertical-align: -0.0833em;"></span><span class="minner">…</span><span class="mspace" style="margin-right: 0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right: 0.2222em;"></span></span><span class="base"><span class="strut" style="height: 0.888em;"></span><span class="mord"><span class="mord">2</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.888em;"><span style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight">n</span><span class="mbin mtight">−</span><span class="mord mtight">1</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 0.7477em; vertical-align: -0.0833em;"></span><span class="mord"><span class="mord">2</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.6644em;"><span style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right: 0.2222em;"></span></span><span class="base"><span class="strut" style="height: 0.6444em;"></span><span class="mord">1</span></span></span></span></p><p>So we need:</p><p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mn>2</mn><mi>n</mi></msup><mo>−</mo><mn>1</mn><mo>≥</mo><mn>100</mn></mrow><annotation encoding="application/x-tex">2^n - 1 \geq 100</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.7477em; vertical-align: -0.0833em;"></span><span class="mord"><span class="mord">2</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.6644em;"><span style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right: 0.2222em;"></span></span><span class="base"><span class="strut" style="height: 0.7804em; vertical-align: -0.136em;"></span><span class="mord">1</span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">≥</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 0.6444em;"></span><span class="mord">100</span></span></span></span></p><p>Solving for <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi></mrow><annotation encoding="application/x-tex">n</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.4306em;"></span><span class="mord mathnormal">n</span></span></span></span>:</p><ol><li><p>Find <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi></mrow><annotation encoding="application/x-tex">n</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.4306em;"></span><span class="mord mathnormal">n</span></span></span></span> such that:</p><p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mn>2</mn><mi>n</mi></msup><mo>≥</mo><mn>101</mn></mrow><annotation encoding="application/x-tex">2^n \geq 101</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.8004em; vertical-align: -0.136em;"></span><span class="mord"><span class="mord">2</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.6644em;"><span style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">≥</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 0.6444em;"></span><span class="mord">101</span></span></span></span></p></li><li><p>Calculate powers of 2 to find the smallest <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi></mrow><annotation encoding="application/x-tex">n</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.4306em;"></span><span class="mord mathnormal">n</span></span></span></span>:</p><ul><li><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mn>2</mn><mn>6</mn></msup><mo>=</mo><mn>64</mn></mrow><annotation encoding="application/x-tex">2^6 = 64</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.8141em;"></span><span class="mord"><span class="mord">2</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.8141em;"><span style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">6</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 0.6444em;"></span><span class="mord">64</span></span></span></span> (not enough)</li><li><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mn>2</mn><mn>7</mn></msup><mo>=</mo><mn>128</mn></mrow><annotation encoding="application/x-tex">2^7 = 128</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.8141em;"></span><span class="mord"><span class="mord">2</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.8141em;"><span style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">7</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 0.6444em;"></span><span class="mord">128</span></span></span></span> (sufficient)</li></ul></li></ol><p>So, <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>=</mo><mn>7</mn></mrow><annotation encoding="application/x-tex">n = 7</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.4306em;"></span><span class="mord mathnormal">n</span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 0.6444em;"></span><span class="mord">7</span></span></span></span> is the smallest integer satisfying <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mn>2</mn><mi>n</mi></msup><mo>≥</mo><mn>101</mn></mrow><annotation encoding="application/x-tex">2^n \geq 101</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.8004em; vertical-align: -0.136em;"></span><span class="mord"><span class="mord">2</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.6644em;"><span style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">≥</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 0.6444em;"></span><span class="mord">101</span></span></span></span>.</p><h3>Conclusion</h3><p>It would take <strong>7 jumps</strong> to climb a 100-storey building if you double your jumping strength with each jump, starting from 1 storey on the first jump.</p></div>
</span>

~~~

Q18: Which all cloud platforms you have used?

Q19: What is Delta Table in Databricks?

<span>
Delta Table is a key feature of Delta Lake, which is an open-source storage layer developed by Databricks to bring ACID (Atomicity, Consistency, Isolation, Durability) transactions and scalable metadata handling to big data workloads. Delta Lake is built on top of existing data lakes (like Apache Spark and Apache Hive) and provides a unified approach to handling streaming and batch data.

Key Features of Delta Tables
ACID Transactions: Delta Tables support ACID transactions, which ensure that all operations are completed successfully or none are applied, maintaining data integrity. This is crucial for handling concurrent writes and reads.

Schema Evolution and Enforcement: Delta Tables allow schema evolution, which means you can modify the schema of the table (add or remove columns) without rewriting the entire dataset. Schema enforcement ensures that the data adheres to the defined schema.

Time Travel: Delta Tables support time travel, which lets you query historical versions of the data. This feature is useful for auditing, data recovery, and reproducing experiments.

Upserts and Deletes: Delta Tables support upserts (merge operations) and deletes, which are not natively supported by traditional data lakes like HDFS. This enables efficient data updates and deletions.

Scalable Metadata Handling: Delta Lake handles metadata at scale efficiently, which is beneficial for large-scale data processing. It avoids the performance bottlenecks associated with traditional data lakes.

Data Optimization: Delta Tables provide built-in data optimization features such as data skipping and file compaction, which improve query performance by reducing the amount of data read.

Delta Table Architecture
Delta Tables are built on top of existing file formats (e.g., Parquet) and add a transaction log to manage changes. The architecture includes:

Data Files: Actual data is stored in Parquet files or other file formats.
Transaction Log: A Delta Log (stored as a series of JSON files) tracks all changes to the table. It contains metadata about the table, including schema changes, additions, deletions, and data modifications.
Checkpoint Files: Periodic snapshots of the transaction log are saved to improve read performance and recovery times.
Basic Operations with Delta Tables
Here’s how you typically interact with Delta Tables using Databricks:

Create a Delta Table:

CREATE TABLE delta_table
USING DELTA
AS SELECT * FROM source_table;

Write Data to a Delta Table:

df.write.format("delta").mode("append").save("/path/to/delta_table")

Read Data from a Delta Table:

df = spark.read.format("delta").load("/path/to/delta_table")

Update Data in a Delta Table:

from delta.tables import DeltaTable

delta_table = DeltaTable.forPath(spark, "/path/to/delta_table")
delta_table.alias("t").merge(
    source_df.alias("s"),
    "t.id = s.id"
).whenMatchedUpdate(set={"value": "s.value"}) \
    .whenNotMatchedInsert(values={"id": "s.id", "value": "s.value"}) \
    .execute()

Time Travel:

df = spark.read.format("delta").option("timestampAsOf", "2023-01-01").load("/path/to/delta_table")

Optimize and Vacuum:

delta_table.optimize()
delta_table.vacuum(retentionHours=168)
Conclusion
Delta Tables provide a robust and flexible framework for managing data in a data lake, combining the advantages of traditional data lakes with features that support ACID transactions, schema evolution, and efficient metadata management. This makes them a powerful tool for handling large-scale data processing and analytics workloads in Databricks.    
</span>
</pre>

<span style="display: none;">Tags: Technology,Machine Learning,Natural Language Processing,Interview Preparation,</span>