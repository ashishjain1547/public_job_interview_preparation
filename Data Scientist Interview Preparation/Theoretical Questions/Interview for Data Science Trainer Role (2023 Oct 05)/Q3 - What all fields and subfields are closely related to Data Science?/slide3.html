<head>
  <script crossorigin="anonymous" integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4="
    src="https://code.jquery.com/jquery-3.2.1.min.js"></script>

  <!-- Google AdSense Using Machine Learning Code -->
  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
    (adsbygoogle = window.adsbygoogle || []).push({
      google_ad_client: "ca-pub-3071098372371409",
      enable_page_level_ads: true
    });
  </script>

  <style>
    pre {
      white-space: pre-wrap;
      white-space: -moz-pre-wrap;
      white-space: -pre-wrap;
      white-space: -o-pre-wrap;
      word-wrap: break-word;
    }
  </style>
</head>

<body>
<pre>Data science is an interdisciplinary academic field that uses statistics, scientific computing, scientific methods, processes, algorithms and systems to extract or extrapolate knowledge and insights from noisy, structured, and unstructured data.

Data science also integrates domain knowledge from the underlying application domain (e.g., natural sciences, information technology, and medicine). Data science is multifaceted and can be described as a science, a research paradigm, a research method, a discipline, a workflow, and a profession.

Data science is a "concept to unify statistics, data analysis, informatics, and their related methods" to "understand and analyze actual phenomena" with data. It uses techniques and theories drawn from many fields within the context of mathematics, statistics, computer science, information science, and domain knowledge. However, data science is different from computer science and information science. Turing Award winner Jim Gray imagined data science as a "fourth paradigm" of science (empirical, theoretical, computational, and now data-driven) and asserted that "everything about science is changing because of the impact of information technology" and the data deluge.

A data scientist is a professional who creates programming code and combines it with statistical knowledge to create insights from data.</pre>

  <h1 style="page-break-before:always; ">
    A list of fields and subfields related to Data Science
  </h1>
  <p>
    1. Data analysis
  </p>
  <p>
    2. Data engineering
  </p>
  <p>
    3. Machine learning
  </p>
  <p>
    4. Business intelligence
  </p>
  <p>
    5. Statistics
  </p>
  <p>
    6. Business analytics
  </p>
  <p>
    7. Software development
  </p>
  <p>
    8. Data mining
  </p>
  <p>
    9. Natural language processing
  </p>
  <p>
    10. Computer vision
  </p>
  <p>
    11. Data storytelling
  </p>
  <p>
    12. Product Management
  </p>
  <p>
    13. Artificial intelligence
  </p>
  <p>
    14. Data modeling
  </p>
  <p>

  </p>
  <p>
    Some Roles That Require Task Related Guidance And Training:
  </p>
  <p>

  </p>
  <p>
    1. Data architect
  </p>
  <p>
    2. Database Administrator
  </p>
  <p>
    3. System Administrator
  </p>
  <p>

  </p>
  <h1 style="page-break-before:always; ">
    1. Data analysis
  </h1>
  <p>
    Data analysis is the process of inspecting, cleansing, transforming, and modeling data with
    the goal of discovering useful information, informing conclusions, and supporting decision-making. Data analysis
    has multiple facets and approaches, encompassing diverse techniques under a variety of names, and is used in
    different business, science, and social science domains. In today's business world, data analysis plays a role in
    making decisions more scientific and helping businesses operate more effectively.
  </p>
  <p>

  </p>
  <p>
    Data mining is a particular data analysis technique that focuses on statistical modeling and
    knowledge discovery for predictive rather than purely descriptive purposes, while business intelligence covers
    data analysis that relies heavily on aggregation, focusing mainly on business information.
  </p>
  <p>

  </p>
  <p>
    In statistical applications, data analysis can be divided into descriptive statistics,
    exploratory data analysis (EDA), and confirmatory data analysis (CDA). EDA focuses on discovering new features in
    the data while CDA focuses on confirming or falsifying existing hypotheses. Predictive analytics focuses on the
    application of statistical models for predictive forecasting or classification, while text analytics applies
    statistical, linguistic, and structural techniques to extract and classify information from textual sources, a
    species of unstructured data. All of the above are varieties of data analysis.
  </p>
  <p>

  </p>
  <p>
    Data integration is a precursor to data analysis, and data analysis is closely linked to data
    visualization and data dissemination.
  </p>
  <h1 style="page-break-before:always; ">
    2. Data engineering
  </h1>
  <p>
    Data engineering refers to the building of systems to enable the collection and usage of data.
    This data is usually used to enable subsequent analysis and data science; which often involves machine learning.
    Making the data usable usually involves substantial compute and storage, as well as data processing.
  </p>
  <p>

  </p>
  <p>
    In the early 2010s, with the rise of the internet, the massive increase in data volumes,
    velocity, and variety led to the term big data to describe the data itself, and data-driven tech companies like
    Facebook and Airbnb started using the phrase data engineer. Due to the new scale of the data, major firms like
    Google, Facebook, Amazon, Apple, Microsoft, and Netflix started to move away from traditional ETL and storage
    techniques. They started creating data engineering, a type of software engineering focused on data, and in
    particular infrastructure, warehousing, data protection, cybersecurity, mining, modelling, processing, and
    metadata management. This change in approach was particularly focused on cloud computing. Data started to be
    handled and used by many parts of the business, such as sales and marketing, and not just IT.
  </p>
  <p>

  </p>
  <p>
    Who is a Data engineer?
  </p>
  <p>
    A data engineer is a type of software engineer who creates big data ETL pipelines to manage
    the flow of data through the organization. This makes it possible to take huge amounts of data and translate it
    into insights. They are focused on the production readiness of data and things like formats, resilience, scaling,
    and security. Data engineers usually hail from a software engineering background and are proficient in programming
    languages like Java, Python, Scala, and Rust. They will be more familiar with databases, architecture, cloud
    computing, and Agile software development.
  </p>
  <p>

  </p>
  <p>
    Who is a Data Scientist?
  </p>
  <p>
    Data scientists are more focused on the analysis of the data, they will be more familiar with
    mathematics, algorithms, statistics, and machine learning.
  </p>
  <h1 style="page-break-before:always; ">
    3. Machine learning
  </h1>
  <p>
    Machine learning (ML) is an umbrella term for solving problems for which development of
    algorithms by human programmers would be cost-prohibitive, and instead the problems are solved by helping machines
    &quot;discover&quot; their &quot;own&quot; algorithms, without needing to be explicitly told what to do by any
    human-developed algorithms. Recently, generative artificial neural networks have been able to surpass results of
    many previous approaches. Machine-learning approaches have been applied to large language models, computer vision,
    speech recognition, email filtering, agriculture and medicine, where it is too costly to develop algorithms to
    perform the needed tasks.
  </p>
  <p>

  </p>
  <p>
    The mathematical foundations of ML are provided by mathematical optimization (mathematical
    programming) methods. Data mining is a related (parallel) field of study, focusing on exploratory data analysis
    through unsupervised learning.
  </p>
  <p>

  </p>
  <p>
    ML is known in its application across business problems under the name predictive analytics.
    Although not all machine learning is statistically based, computational statistics is an important source of the
    field's methods.
  </p>
  <p>

  </p>
  <p>

  </p>
  <h1 style="page-break-before:always; ">
    Machine learning approaches
  </h1>
  <p>
    Machine learning approaches are traditionally divided into three broad categories, which
    correspond to learning paradigms, depending on the nature of the &quot;signal&quot; or &quot;feedback&quot;
    available to the learning system:
  </p>
  <p>

  </p>
  <p>
    Supervised learning: The computer is presented with example inputs and their desired outputs,
    given by a &quot;teacher&quot;, and the goal is to learn a general rule that maps inputs to outputs.
  </p>
  <p>

  </p>
  <p>
    Unsupervised learning: No labels are given to the learning algorithm, leaving it on its own
    to find structure in its input. Unsupervised learning can be a goal in itself (discovering hidden patterns in
    data) or a means towards an end (feature learning).
  </p>
  <p>

  </p>
  <p>
    Reinforcement learning: A computer program interacts with a dynamic environment in which it
    must perform a certain goal (such as driving a vehicle or playing a game against an opponent). As it navigates its
    problem space, the program is provided feedback that's analogous to rewards, which it tries to maximize. Although
    each algorithm has advantages and limitations, no single algorithm works for all problems.
  </p>
  <h1 style="page-break-before:always; ">
    4. Business intelligence
  </h1>
  <p>
    Business intelligence (BI) comprises the strategies and technologies used by enterprises for
    the data analysis and management of business information. Common functions of business intelligence technologies
    include reporting, online analytical processing, analytics, dashboard development, data mining, process mining,
    complex event processing, business performance management, benchmarking, text mining, predictive analytics, and
    prescriptive analytics.
  </p>
  <p>

  </p>
  <p>
    BI tools can handle large amounts of structured and sometimes unstructured data to help
    identify, develop, and otherwise create new strategic business opportunities. They aim to allow for the easy
    interpretation of these big data. Identifying new opportunities and implementing an effective strategy based on
    insights can provide businesses with a competitive market advantage and long-term stability, and help them take
    strategic decisions.
  </p>
  <p>

  </p>
  <p>
    Business intelligence can be used by enterprises to support a wide range of business decisions
    ranging from operational to strategic. Basic operating decisions include product positioning or pricing. Strategic
    business decisions involve priorities, goals, and directions at the broadest level. In all cases, BI is most
    effective when it combines data derived from the market in which a company operates (external data) with data from
    company sources internal to the business such as financial and operations data (internal data). When combined,
    external and internal data can provide a complete picture which, in effect, creates an &quot;intelligence&quot;
    that cannot be derived from any singular set of data.
  </p>
  <p>

  </p>
  <p>
    Among myriad uses, business intelligence tools empower organizations to gain insight into new
    markets, to assess demand and suitability of products and services for different market segments, and to gauge the
    impact of marketing efforts.
  </p>
  <p>

  </p>
  <p>
    BI applications use data gathered from a data warehouse (DW) or from a data mart, and the
    concepts of BI and DW combine as &quot;BI/DW&quot; or as &quot;BIDW&quot;. A data warehouse contains a copy of
    analytical data that facilitates decision support.
  </p>
  <h1 style="page-break-before:always; ">
    Definition of ‘Business Intelligence’
  </h1>
  <p>
    According to Solomon Negash and Paul Gray, business intelligence (BI) can be defined as
    systems that combine:
  </p>
  <p>

  </p>
  <p>
    1. Data gathering
  </p>
  <p>
    2. Data storage
  </p>
  <p>
    3. Knowledge management
  </p>
  <br>
  <p>
    Some elements of business intelligence are:
  </p>
  <p>

  </p>
  <p>
    1. Multidimensional aggregation and allocation
  </p>
  <p>
    2. Denormalization, tagging, and standardization
  </p>
  <p>
    3. Realtime reporting with analytical alert
  </p>
  <p>
    4. A method of interfacing with unstructured data sources
  </p>
  <p>
    5. Group consolidation, budgeting, and rolling forecasts
  </p>
  <p>
    6. Statistical inference and probabilistic simulation
  </p>
  <p>
    7. Key performance indicators optimization
  </p>
  <p>
    8. Version control and process management
  </p>
  <p>
    9. Open item management
  </p>
  <h1 style="page-break-before:always; ">
    Roles in the field of ‘Business Intelligence’
  </h1>
  <p>
    Some common technical roles for business intelligence developers are:
  </p>
  <p>

  </p>
  <p>
    # Business analyst
  </p>
  <p>
    # Data analyst
  </p>
  <p>
    # Data engineer
  </p>
  <p>
    # Data scientist
  </p>
  <p>
    # Database administrator
  </p>
  <h1 style="page-break-before:always; ">
    5. Statistics (and Statisticians)
  </h1>
  <p><b>
      Statistics
    </b> is a mathematical body of science that pertains to the collection, analysis, interpretation or explanation, and
    presentation of data, or as a branch of mathematics. Some consider statistics to be a distinct mathematical science
    rather than a branch of mathematics. While many scientific investigations make use of data, statistics is concerned
    with the use of data in the context of uncertainty and decision-making in the face of uncertainty.
  </p>
  <p>

  </p>
  <p>
    A <b>statistician</b> is a person who works with theoretical or applied statistics. The
    profession exists in both the private and public sectors.
  </p>
  <p>
    It is common to combine statistical knowledge with expertise in other subjects, and
    statisticians may work as employees or as statistical consultants.
  </p>
  <h1 style="page-break-before:always; ">
    6. Business analytics
  </h1>
  <p>
    Business analytics (BA) refers to the skills, technologies, and practices for iterative
    exploration and investigation of past business performance to gain insight and drive business planning. Business
    analytics focuses on developing new insights and understanding of business performance based on data and
    statistical methods. In contrast, business intelligence traditionally focuses on using a consistent set of metrics
    to both measure past performance and guide business planning. In other words, business intelligence focusses on
    description, while business analytics focusses on prediction and prescription.
  </p>
  <p>

  </p>
  <p>
    Business analytics makes extensive use of analytical modeling and numerical analysis,
    including explanatory and predictive modeling, and fact-based management to drive decision making. It is therefore
    closely related to management science. Analytics may be used as input for human decisions or may drive fully
    automated decisions. Business intelligence is querying, reporting, online analytical processing (OLAP), and
    &quot;alerts&quot;.
  </p>
  <p>

  </p>
  <p>
    In other words, querying, reporting, and OLAP are alert tools that can answer questions such
    as what happened, how many, how often, where the problem is, and what actions are needed. Business analytics can
    answer questions like why is this happening, what if these trends continue, what will happen next (predict), and
    what is the best outcome that can happen (optimize).
  </p>
  <h1 style="page-break-before:always; ">
    7. Software development
  </h1>
  <p>
    Software development is the process used to conceive, specify, design, program, document,
    test, and bug fix in order to create and maintain applications, frameworks, or other software components. Software
    development involves writing and maintaining the source code, but in a broader sense, it includes all processes
    from the conception of the desired software through the final manifestation, typically in a planned and structured
    process often overlapping with software engineering. Software development also includes research, new development,
    prototyping, modification, reuse, re-engineering, maintenance, or any other activities that result in software
    products.
  </p>
  <h1 style="page-break-before:always; ">
    8. Data mining
  </h1>
  <p>
    Data mining is the process of extracting and discovering patterns in large data sets involving
    methods at the intersection of machine learning, statistics, and database systems. Data mining is an
    interdisciplinary subfield of computer science and statistics with an overall goal of extracting information (with
    intelligent methods) from a data set and transforming the information into a comprehensible structure for further
    use. Data mining is the analysis step of the &quot;knowledge discovery in databases&quot; process, or KDD. Aside
    from the raw analysis step, it also involves database and data management aspects, data pre-processing, model and
    inference considerations, interestingness metrics, complexity considerations, post-processing of discovered
    structures, visualization, and online updating.
  </p>
  <p>

  </p>
  <p>
    The term &quot;data mining&quot; is a misnomer because the goal is the extraction of patterns
    and knowledge from large amounts of data, not the extraction (mining) of data itself. It also is a buzzword and is
    frequently applied to any form of large-scale data or information processing (collection, extraction, warehousing,
    analysis, and statistics) as well as any application of computer decision support system, including artificial
    intelligence (e.g., machine learning) and business intelligence. The book Data Mining: Practical Machine Learning
    Tools and Techniques with Java (which covers mostly machine learning material) was originally to be named
    Practical Machine Learning, and the term data mining was only added for marketing reasons. Often the more general
    terms (large scale) data analysis and analytics—or, when referring to actual methods, artificial intelligence and
    machine learning—are more appropriate.
  </p>
  <p>

  </p>
  <p>
    The actual data mining task is the semi-automatic or automatic analysis of large quantities of
    data to extract previously unknown, interesting patterns such as groups of data records (cluster analysis),
    unusual records (anomaly detection), and dependencies (association rule mining, sequential pattern mining). This
    usually involves using database techniques such as spatial indices. These patterns can then be seen as a kind of
    summary of the input data, and may be used in further analysis or, for example, in machine learning and predictive
    analytics. For example, the data mining step might identify multiple groups in the data, which can then be used to
    obtain more accurate prediction results by a decision support system. Neither the data collection, data
    preparation, nor result interpretation and reporting is part of the data mining step, although they do belong to
    the overall KDD process as additional steps.
  </p>
  <p>

  </p>
  <p><b>
      The difference between data analysis and data mining is that data analysis is used to test
      models and hypotheses on the dataset, e.g., analyzing the effectiveness of a marketing campaign, regardless of
      the amount of data. In contrast, data mining uses machine learning and statistical models to uncover clandestine
      or hidden patterns in a large volume of data.
    </b>
  </p>
  <p><b>

    </b>
  </p>
  <p>
    The related terms data dredging, data fishing, and data snooping refer to the use of data
    mining methods to sample parts of a larger population data set that are (or may be) too small for reliable
    statistical inferences to be made about the validity of any patterns discovered. These methods can, however, be
    used in creating new hypotheses to test against the larger data populations.
  </p>
  <h1 style="page-break-before:always; ">
    Data Mining Process
  </h1>
  <p>
    The knowledge discovery in databases (KDD) process is commonly defined with the stages:
  </p>
  <p>

  </p>
  <p>
    1. Selection
  </p>
  <p>
    2. Pre-processing
  </p>
  <p>
    3. Transformation
  </p>
  <p>
    4. Data mining
  </p>
  <p>
    5. Interpretation/evaluation.
  </p>
  <p>

  </p>
  <p>
    It exists, however, in many variations on this theme, such as the <b>Cross-industry standard
      process for data mining (CRISP-DM)</b> which defines six phases:
  </p>
  <p>

  </p>
  <p>
    1. Business understanding
  </p>
  <p>
    2. Data understanding
  </p>
  <p>
    3. Data preparation
  </p>
  <p>
    4. Modeling
  </p>
  <p>
    5. Evaluation
  </p>
  <p>
    6. Deployment
  </p>
  <p>

  </p>
  <p>
    or a simplified process such as (1) Pre-processing, (2) Data Mining, and (3) Results
    Validation.
  </p>
  <p>

  </p>
  <p>
    Polls conducted in 2002, 2004, 2007 and 2014 show that the CRISP-DM methodology is the leading
    methodology used by data miners. The only other data mining standard named in these polls was SEMMA. However, 3–4
    times as many people reported using CRISP-DM. Several teams of researchers have published reviews of data mining
    process models, and Azevedo and Santos conducted a comparison of CRISP-DM and SEMMA in 2008.
  </p>
  <p>
    Note: SEMMA is an acronym that stands for Sample, Explore, Modify, Model, and Assess.
  </p>
  <h1 style="page-break-before:always; ">
    9. Natural language processing
  </h1>
  <p>
    Natural language processing (NLP) is an interdisciplinary subfield of computer science and
    linguistics. It is primarily concerned with giving computers the ability to support and manipulate speech. It
    involves processing natural language datasets, such as text corpora or speech corpora, using either rule-based or
    probabilistic (i.e. statistical and, most recently, neural network-based) machine learning approaches. The goal is
    a computer capable of &quot;understanding&quot; the contents of documents, including the contextual nuances of the
    language within them. The technology can then accurately extract information and insights contained in the
    documents as well as categorize and organize the documents themselves.
  </p>
  <p>

  </p>
  <p>
    Challenges in natural language processing frequently involve speech recognition,
    natural-language understanding, and natural-language generation.
  </p>
  <h1 style="page-break-before:always; ">
    10. Computer vision
  </h1>
  <p>
    Computer vision tasks include methods for acquiring, processing, analyzing and understanding
    digital images, and extraction of high-dimensional data from the real world in order to produce numerical or
    symbolic information, e.g. in the forms of decisions. Understanding in this context means the transformation of
    visual images (the input to the retina in the human analog) into descriptions of the world that make sense to
    thought processes and can elicit appropriate action. This image understanding can be seen as the disentangling of
    symbolic information from image data using models constructed with the aid of geometry, physics, statistics, and
    learning theory.
  </p>
  <p>

  </p>
  <p>
    The scientific discipline of computer vision is concerned with the theory behind artificial
    systems that extract information from images. The image data can take many forms, such as video sequences, views
    from multiple cameras, multi-dimensional data from a 3D scanner, 3D point clouds from LiDaR sensors, or medical
    scanning devices. The technological discipline of computer vision seeks to apply its theories and models to the
    construction of computer vision systems.
  </p>
  <p>

  </p>
  <p>
    Sub-domains of computer vision include scene reconstruction, object detection, event
    detection, activity recognition, video tracking, object recognition, 3D pose estimation, learning, indexing,
    motion estimation, visual servoing, 3D scene modeling, and image restoration.
  </p>
  <p>

  </p>
  <p>
    Adopting computer vision technology might be painstaking for organizations as there is no
    single point solution for it. There are very few companies that provide a unified and distributed platform or an
    Operating System where computer vision applications can be easily deployed and managed.
  </p>
  <h1 style="page-break-before:always; ">
    11. Data storytelling
  </h1>
  <p>
    Data storytellers visualize data, create reports, search for narratives that best characterize
    data, and design innovative methods to convey that narrative. Data storytelling is a creative job role that falls
    in between data analysis and human-centered communication. They reduce the data to focus on a certain feature,
    evaluate the behavior, and create a story that assists others in better understanding business trends.
  </p>
  <div class="separator" style="clear: both;"><a
      href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgDOXXbN_4GZcbbI7ZNlPykLiMOblygcq-xSKEfRtRUxCMoMBhtTnGHjPOgJ_bDtluNqLQJ0sxg8aXaKJyvRb76M87_s4QXB7SQ9FcWS0T-vyg8FQuhoqj-cZE6fQWT70EGgTtU36ZIhgHB-OZW4umGNKR75y3HBh1G_9ojINa5imoLHH_94Uy96iyfiRw3/s943/Screenshot%20from%202023-10-02%2015-21-23.png"
      style="display: block; padding: 1em 0; text-align: center; "><img alt="" border="0" width="600"
        data-original-height="513" data-original-width="943"
        src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgDOXXbN_4GZcbbI7ZNlPykLiMOblygcq-xSKEfRtRUxCMoMBhtTnGHjPOgJ_bDtluNqLQJ0sxg8aXaKJyvRb76M87_s4QXB7SQ9FcWS0T-vyg8FQuhoqj-cZE6fQWT70EGgTtU36ZIhgHB-OZW4umGNKR75y3HBh1G_9ojINa5imoLHH_94Uy96iyfiRw3/s600/Screenshot%20from%202023-10-02%2015-21-23.png" /></a>
  </div>
  <h1 style="page-break-before:always; ">
    12. Product Management
  </h1>
  <p>
    Product management is the business process of planning, developing, launching, and managing a
    product or service. It includes the entire lifecycle of a product, from ideation to development to go to market.
    Product managers are responsible for ensuring that a product meets the needs of its target market and contributes
    to the business strategy, while managing a product or products at all stages of the product lifecycle. Software
    product management adapts the fundamentals of product management for digital products.
  </p>
  <h1 style="page-break-before:always; ">
    13. Artificial intelligence
  </h1>
  <p>
    Artificial intelligence (AI) is the intelligence of machines or software, as opposed to the
    intelligence of humans or animals. It is also the field of study in computer science that develops and studies
    intelligent machines. &quot;AI&quot; may also refer to the machines themselves.
  </p>
  <p>

  </p>
  <p>
    AI technology is widely used throughout industry, government and science. Some high-profile
    applications are: advanced web search engines (e.g., Google Search), recommendation systems (used by YouTube,
    Amazon, and Netflix), understanding human speech (such as Siri and Alexa), self-driving cars (e.g., Waymo),
    generative or creative tools (ChatGPT and AI art), and competing at the highest level in strategic games (such as
    chess and Go).
  </p>
  <p>

  </p>
  <p>
    Artificial intelligence was founded as an academic discipline in 1956. The field went through
    multiple cycles of optimism followed by disappointment and loss of funding, but after 2012, when deep learning
    surpassed all previous AI techniques, there was a vast increase in funding and interest.
  </p>
  <p>

  </p>
  <p>
    The various sub-fields of AI research are centered around particular goals and the use of
    particular tools. The traditional goals of AI research include reasoning, knowledge representation, planning,
    learning, natural language processing, perception, and support for robotics. General intelligence (the ability to
    solve an arbitrary problem) is among the field's long-term goals. To solve these problems, AI researchers have
    adapted and integrated a wide range of problem-solving techniques, including search and mathematical optimization,
    formal logic, artificial neural networks, and methods based on statistics, probability, and economics. AI also
    draws upon psychology, linguistics, philosophy, neuroscience and many other fields.
  </p>
  <h1 style="page-break-before:always; ">
    14. Data Modeling
  </h1>
  <p>
    Data modeling is a process used to define and analyze data requirements needed to support the
    business processes within the scope of corresponding information systems in organizations. Therefore, the process
    of data modeling involves professional data modelers working closely with business stakeholders, as well as
    potential users of the information system.
  </p>
  <p>

  </p>
  <p>
    There are three different types of data models produced while progressing from requirements to
    the actual database to be used for the information system. The data requirements are initially recorded as a
    conceptual data model which is essentially a set of technology independent specifications about the data and is
    used to discuss initial requirements with the business stakeholders. The conceptual model is then translated into
    a logical data model, which documents structures of the data that can be implemented in databases. Implementation
    of one conceptual data model may require multiple logical data models. The last step in data modeling is
    transforming the logical data model to a physical data model that organizes the data into tables, and accounts for
    access, performance and storage details. Data modeling defines not just data elements, but also their structures
    and the relationships between them.
  </p>
  <p>

  </p>
  <p>
    Data modeling techniques and methodologies are used to model data in a standard, consistent,
    predictable manner in order to manage it as a resource. The use of data modeling standards is strongly recommended
    for all projects requiring a standard means of defining and analyzing data within an organization, e.g., using
    data modeling:
  </p>
  <p>

  </p>
  <p>
    # to assist business analysts, programmers, testers, manual writers, IT package selectors,
    engineers, managers, related organizations and clients to understand and use an agreed upon semi-formal model that
    encompasses the concepts of the organization and how they relate to one another
  </p>
  <p>
    # to manage data as a resource
  </p>
  <p>
    # to integrate information systems
  </p>
  <p>
    # to design databases/data warehouses (aka data repositories)
  </p>
  <p>

  </p>
  <p>
    Data modeling may be performed during various types of projects and in multiple phases of
    projects. Data models are progressive; there is no such thing as the final data model for a business or
    application. Instead a data model should be considered a living document that will change in response to a
    changing business. The data models should ideally be stored in a repository so that they can be retrieved,
    expanded, and edited over time. Whitten et al. (2004) determined two types of data modeling:
  </p>
  <p>

  </p>
  <p>
    # Strategic data modeling: This is part of the creation of an information systems strategy,
    which defines an overall vision and architecture for information systems. Information technology engineering is a
    methodology that embraces this approach.
  </p>
  <p>
    # Data modeling during systems analysis: In systems analysis logical data models are created
    as part of the development of new databases.
  </p>
  <p>

  </p>
  <p>
    Data modeling is also used as a technique for detailing business requirements for specific
    databases. It is sometimes called database modeling because a data model is eventually implemented in a database.

  </p>
</body>

<span style="display: none;">Tags: Data Analytics,Technology,Interview Preparation,</span>